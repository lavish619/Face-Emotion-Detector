{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from models import trial_model, alexnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"./facial_expressions/image_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facial-expressions_2868585k.jpg</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facial-expressions_2868584k.jpg</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facial-expressions_2868582k.jpg</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron_Eckhart_0001.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron_Guiel_0001.jpg</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13684</th>\n",
       "      <td>SharmilaTagore_80.jpg</td>\n",
       "      <td>HAPPINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>SharmilaTagore_81.jpg</td>\n",
       "      <td>HAPPINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13686</th>\n",
       "      <td>SharmilaTagore_82.jpg</td>\n",
       "      <td>HAPPINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13687</th>\n",
       "      <td>SharmilaTagore_83.jpg</td>\n",
       "      <td>HAPPINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13688</th>\n",
       "      <td>SharmilaTagore_9.jpg</td>\n",
       "      <td>HAPPINESS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13689 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 image    emotion\n",
       "0      facial-expressions_2868585k.jpg   surprise\n",
       "1      facial-expressions_2868584k.jpg    disgust\n",
       "2      facial-expressions_2868582k.jpg       fear\n",
       "3               Aaron_Eckhart_0001.jpg    neutral\n",
       "4                 Aaron_Guiel_0001.jpg  happiness\n",
       "...                                ...        ...\n",
       "13684            SharmilaTagore_80.jpg  HAPPINESS\n",
       "13685            SharmilaTagore_81.jpg  HAPPINESS\n",
       "13686            SharmilaTagore_82.jpg  HAPPINESS\n",
       "13687            SharmilaTagore_83.jpg  HAPPINESS\n",
       "13688             SharmilaTagore_9.jpg  HAPPINESS\n",
       "\n",
       "[13689 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      6717\n",
       "happiness    5309\n",
       "HAPPINESS     387\n",
       "surprise      356\n",
       "anger         227\n",
       "DISGUST       195\n",
       "NEUTRAL       151\n",
       "SADNESS       144\n",
       "sadness       124\n",
       "ANGER          24\n",
       "fear           13\n",
       "disgust        13\n",
       "SURPRISE       12\n",
       "contempt        9\n",
       "FEAR            8\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      6868\n",
       "happiness    5696\n",
       "surprise      368\n",
       "sadness       268\n",
       "anger         251\n",
       "disgust       208\n",
       "fear           21\n",
       "contempt        9\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing emotion in capital to small\n",
    "dataset['emotion'] = dataset['emotion'].replace({'NEUTRAL':'neutral','HAPPINESS':'happiness','DISGUST':'disgust',\n",
    "                                                 'SADNESS':'sadness','ANGER':'anger','SURPRISE':'surprise',\n",
    "                                                 'FEAR':'fear'})\n",
    "\n",
    "num_outputs = len(dataset[\"emotion\"].value_counts()) #number of classes\n",
    "dataset[\"emotion\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset['image'], dataset['emotion'])\n",
    "\n",
    "train_data = pd.concat([x_train, y_train],axis = 1)  #training dataframe\n",
    "test_data = pd.concat([x_test, y_test],axis = 1)  #test dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8213 validated image filenames belonging to 8 classes.\n",
      "Found 2053 validated image filenames belonging to 8 classes.\n",
      "Found 3423 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split = 0.2) # rescaling the pixel values between 0 to 1.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # rescaling the pixel values between 0 to 1.\n",
    "\n",
    "#Access training images from directory marked to their class labels in dataframe\n",
    "train_generator = datagen.flow_from_dataframe(dataframe = train_data,\n",
    "                                              directory= \"./facial_expressions/images\",\n",
    "                                              x_col=\"image\", y_col = \"emotion\",\n",
    "                                              target_size = (350,350),\n",
    "                                              subset = \"training\", batch_size = 64)  \n",
    "\n",
    "#Access validation images from directory marked to their class labels in dataframe\n",
    "validation_generator = datagen.flow_from_dataframe(dataframe = train_data,\n",
    "                                                   directory= \"./facial_expressions/images\",\n",
    "                                                   x_col=\"image\", y_col = \"emotion\",\n",
    "                                                   target_size = (350,350),\n",
    "                                                   subset= \"validation\",batch_size = 64)\n",
    "\n",
    "#Access test images from directory marked to their class labels in dataframe\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe = test_data,\n",
    "                                                  directory= \"./facial_expressions/images\",\n",
    "                                                  x_col=\"image\", y_col = \"emotion\",\n",
    "                                                  target_size = (350,350))\n",
    "\n",
    "#traget size is chosen (350,350) because maximum images have this dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'contempt': 1,\n",
       " 'disgust': 2,\n",
       " 'fear': 3,\n",
       " 'happiness': 4,\n",
       " 'neutral': 5,\n",
       " 'sadness': 6,\n",
       " 'surprise': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices = train_generator.class_indices #get indices of each class in one hot encoding done during flow from dataframe\n",
    "label_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 85, 85, 48)        17472     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 85, 85, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 85, 85, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 42, 42, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 128)       153728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 42, 42, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 192)       221376    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 192)       331968    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20, 20, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 128)       221312    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              10617856  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 8200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 12,630,440\n",
      "Trainable params: 12,625,976\n",
      "Non-trainable params: 4,464\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = alexnet_model((350,350,3), num_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 2.1042 - accuracy: 0.3137 - val_loss: 4.7271 - val_accuracy: 0.0122\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 63s 625ms/step - loss: 1.7583 - accuracy: 0.5175 - val_loss: 1.1962 - val_accuracy: 0.5558\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 1.4639 - accuracy: 0.6775 - val_loss: 1.4257 - val_accuracy: 0.5363\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 18s 183ms/step - loss: 1.3882 - accuracy: 0.6587 - val_loss: 1.3166 - val_accuracy: 0.6069\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 19s 189ms/step - loss: 1.3177 - accuracy: 0.6637 - val_loss: 1.4687 - val_accuracy: 0.5343\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 20s 199ms/step - loss: 1.2085 - accuracy: 0.7212 - val_loss: 1.1688 - val_accuracy: 0.6405\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 22s 217ms/step - loss: 1.0671 - accuracy: 0.7337 - val_loss: 1.5607 - val_accuracy: 0.4983\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 1.0280 - accuracy: 0.7412 - val_loss: 2.6734 - val_accuracy: 0.4131\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 0.9758 - accuracy: 0.7325 - val_loss: 1.5864 - val_accuracy: 0.4720\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 21s 205ms/step - loss: 0.9464 - accuracy: 0.7425 - val_loss: 0.9724 - val_accuracy: 0.6620\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 0.8724 - accuracy: 0.7538 - val_loss: 1.1657 - val_accuracy: 0.5952\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 50s 497ms/step - loss: 0.9519 - accuracy: 0.7362 - val_loss: 1.2698 - val_accuracy: 0.6683\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 52s 521ms/step - loss: 0.8519 - accuracy: 0.7591 - val_loss: 0.8694 - val_accuracy: 0.7599\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.8235 - accuracy: 0.7638 - val_loss: 0.8011 - val_accuracy: 0.7691\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.8503 - accuracy: 0.7675 - val_loss: 0.7916 - val_accuracy: 0.7540\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.7267 - accuracy: 0.7937 - val_loss: 0.7503 - val_accuracy: 0.7759\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.8033 - accuracy: 0.7688 - val_loss: 0.9443 - val_accuracy: 0.7565\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.7571 - accuracy: 0.7800 - val_loss: 0.8288 - val_accuracy: 0.7180\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.7288 - accuracy: 0.7725 - val_loss: 0.7041 - val_accuracy: 0.7764\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 105s 1s/step - loss: 0.6814 - accuracy: 0.8025 - val_loss: 0.6827 - val_accuracy: 0.7915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x209d42bc508>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=100,\n",
    "          epochs = 20,\n",
    "          validation_data = validation_generator\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 66s 617ms/step - loss: 0.6537 - accuracy: 0.8008\n",
      "Accuracy on test set: 0.8007595539093018\n"
     ]
    }
   ],
   "source": [
    "# get accuracy on test set\n",
    "pred = model.evaluate(test_generator)\n",
    "print(\"Accuracy on test set:\", pred[1])\n",
    "\n",
    "# save the trained weights\n",
    "model.save_weights(\"alexnet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
